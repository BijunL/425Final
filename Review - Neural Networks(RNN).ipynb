{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "C4tARjO8iMgz"
   },
   "outputs": [],
   "source": [
    "# 定义一个简单的循环神经网络（RNN）封装类，旨在为连续和离散输出提供灵活的预测模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "循环神经网络的英文全称是 \"Recurrent Neural Network\" (RNN)。RNN 是一种强大的神经网络模型，专门设计来处理序列数据，如时间序列数据、自然语言文本、语音等。它的关键特性是网络内部存在循环，使得网络能够在处理序列的每个元素时保持信息的持续流动。\n",
    "\n",
    "### RNN 原理\n",
    "\n",
    "1. **循环结构（Recurrent Structure）**：\n",
    "   RNN 的每个节点（或单元）接收两个输入：当前时间步（Time Step）的输入数据和来自上一个时间步的隐状态（Hidden State）。这种循环结构允许网络维护并更新一个内部状态，该状态编码了到目前为止观察到的序列中的信息。\n",
    "\n",
    "2. **隐状态（Hidden State）**：\n",
    "   隐状态是 RNN 的核心，它是网络的内部记忆（Internal Memory），用于存储关于已处理序列的信息。在每个时间步，隐状态会根据当前的输入和上一时间步的隐状态进行更新。\n",
    "\n",
    "3. **参数共享（Parameter Sharing）**：\n",
    "   RNN 在处理序列的每个时间步时共享同一组参数（权重和偏置）。这意味着无论输入序列有多长，学习的模型参数数量都保持不变。参数共享提高了模型的数据效率和可扩展性。\n",
    "\n",
    "4. **时间步循环（Time Step Recurrence）**：\n",
    "   在处理序列时，RNN 会在每个时间步重复相同的任务，但每次都会更新其隐状态。这种时间步循环允许 RNN 逐步构建对整个序列的理解。\n",
    "\n",
    "### RNN 的局限性\n",
    "\n",
    "尽管 RNN 在理论上能够处理任意长度的序列，但在实践中，它们面临一些挑战：\n",
    "\n",
    "1. **梯度消失和爆炸（Vanishing & Exploding Gradients）**：\n",
    "   在长序列中，RNN 很难学习长距离依赖关系，这主要是因为在训练过程中梯度可能会急剧减小（消失）或增大（爆炸），导致网络难以学习。\n",
    "\n",
    "2. **顺序依赖（Sequential Dependency）**：\n",
    "   由于 RNN 的循环结构，必须按顺序处理序列中的每个元素，这限制了模型训练和推断的并行性。\n",
    "\n",
    "### RNN 的改进\n",
    "\n",
    "为了克服标准 RNN 的这些限制，研究人员提出了几种改进的 RNN 结构，如：\n",
    "\n",
    "- **长短期记忆网络（Long Short-Term Memory, LSTM）**：\n",
    "  LSTM 通过引入一种复杂的门控机制（Gating Mechanism）来控制信息的流入、流出和遗忘，从而更有效地捕获长距离依赖关系。\n",
    "\n",
    "- **门控循环单元（Gated Recurrent Unit, GRU）**：\n",
    "  GRU 是 LSTM 的一个变体，具有更简化的结构，但仍然通过类似的门控机制来管理信息流，提高了处理长序列的能力。\n",
    "\n",
    "RNN 及其变种在多个领域展现出了强大的性能，使得它们成为处理序列数据时的重要工具。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRkewr3TiOn8"
   },
   "source": [
    "# 1.) Create a function that takes X, y data and allows you to fit/predict similar to a scikit learn function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "tzaZEHcqiYm0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 类定义和初始化\n",
    "\n",
    "# SimpleRNNWrapper类的构造函数初始化模型的基本参数，\n",
    "# 包括输出类型（连续或离散）、时间序列滞后观察数（lags）、RNN层的单元数、训练的轮数（epochs）和批处理大小（batch_size）。\n",
    "class SimpleRNNWrapper:\n",
    "    def __init__(self, output_type='continuous', lags=1, rnn_units=50, epochs=10, batch_size=32):\n",
    "        \"\"\"\n",
    "        Initialize the RNN wrapper.\n",
    "\n",
    "        Parameters:\n",
    "        - output_type: 'continuous' for regression tasks, 'discrete' for classification.\n",
    "        - lags: Number of lag observations to use as input features.\n",
    "        - rnn_units: Number of units in the RNN layer.\n",
    "        - epochs: Number of epochs to train the model.\n",
    "        - batch_size: Batch size for training.\n",
    "        \"\"\"\n",
    "        self.output_type = output_type\n",
    "        self.lags = lags\n",
    "        self.rnn_units = rnn_units\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.model = None\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.label_encoder = None\n",
    "        if output_type == 'discrete':\n",
    "            self.label_encoder = LabelEncoder() \n",
    "            # 根据输出类型（连续或离散），可能还会初始化一个标签编码器（LabelEncoder）\n",
    "\n",
    "\n",
    "# 数据预处理\n",
    "            \n",
    "# _preprocess_data方法：负责数据预处理，包括特征缩放（使用MinMaxScaler）和将数据结构化为序列形式，以适应RNN的输入需求       \n",
    "    def _preprocess_data(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Preprocess the data: scale features, encode labels (if discrete), and structure into sequences.\n",
    "        \"\"\"\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        X_seq = np.array([X_scaled[i:i+self.lags] for i in range(len(X_scaled)-self.lags)])\n",
    "\n",
    "        if y is not None:\n",
    "            if self.output_type == 'continuous':\n",
    "                y_seq = y[self.lags:]\n",
    "            else:  # Discrete\n",
    "                y_encoded = self.label_encoder.fit_transform(y)\n",
    "                y_seq = to_categorical(y_encoded[self.lags:])\n",
    "                # 对于离散输出，还会对目标变量进行编码（使用LabelEncoder）并转换为独热编码（使用to_categorical）\n",
    "            return X_seq, y_seq\n",
    "\n",
    "        return X_seq\n",
    "\n",
    "\n",
    "# 模型构建   \n",
    "# _build_model方法：根据初始化时指定的参数构建RNN模型。模型由一个简单的RNN层（SimpleRNN）和一个输出层组成。\n",
    "    def _build_model(self, input_shape, output_shape):\n",
    "        \"\"\"\n",
    "        Build the RNN model based on specified parameters.\n",
    "        \"\"\"\n",
    "        self.model = Sequential()\n",
    "        self.model.add(SimpleRNN(self.rnn_units, input_shape=input_shape))\n",
    "        if self.output_type == 'continuous':\n",
    "            self.model.add(Dense(1))\n",
    "        else:  # Discrete\n",
    "            self.model.add(Dense(output_shape, activation='softmax'))\n",
    "            # 输出层对于连续任务使用一个单元（无激活函数），对于离散任务则使用与目标类别数相同的单元数，并采用softmax激活函数。\n",
    "\n",
    "        self.model.compile(optimizer='adam', loss='mse' if self.output_type == 'continuous' else 'categorical_crossentropy', metrics=['accuracy'])\n",
    "        # 损失函数根据任务类型（连续或离散）分别设置为均方误差（mse）或分类交叉熵（categorical_crossentropy）\n",
    "        \n",
    "        \n",
    "\n",
    "# 模型训练\n",
    "# fit方法：负责模型训练。\n",
    "# 首先，它会调用_preprocess_data方法对输入数据进行预处理，然后调用_build_model构建模型，并使用预处理后的数据训练模型。\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the RNN model to the data.\n",
    "        \"\"\"\n",
    "        X_seq, y_seq = self._preprocess_data(X, y)\n",
    "        self._build_model(input_shape=(X_seq.shape[1], X_seq.shape[2]), output_shape=y_seq.shape[1])\n",
    "        self.model.fit(X_seq, y_seq, epochs=self.epochs, batch_size=self.batch_size)\n",
    "\n",
    "\n",
    "# 模型预测\n",
    "# predict方法：用于模型预测。\n",
    "# 它首先调用_preprocess_data方法对输入数据X进行预处理，然后使用训练好的模型进行预测。\n",
    "# 对于离散输出，预测结果会经过LabelEncoder的逆转换，以还原为原始的标签。     \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict using the trained RNN model.\n",
    "        \"\"\"\n",
    "        X_seq = self._preprocess_data(X)\n",
    "        predictions = self.model.predict(X_seq)\n",
    "        if self.output_type == 'discrete':\n",
    "            return self.label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "        return predictions.flatten()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4uC4M65riZyg"
   },
   "source": [
    "总结：\n",
    "这个SimpleRNNWrapper类提供了一个封装好的RNN模型，适用于处理序列数据的回归（连续输出）和分类（离散输出）任务。通过这种封装方式，用户可以轻松地在自己的数据集上应用RNN，而无需关心数据预处理和模型构建的复杂细节。此外，这个类还处理了数据的标准化和标签编码，确保数据格式适合神经网络，并在预测时还原了原始的标签，增强了模型的易用性和可解释性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GxGpOZ_ilOcQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Principal Component Analysis（PCA）\n",
        "\n",
        "主成分分析（Principal Component Analysis, PCA）是一种统计程序，它通过一组较小的“概要指标”来总结大型数据表中的信息内容，这些概要指标可以更容易地进行可视化和分析。底层数据可以是描述生产样本、化学化合物或反应、连续过程的过程时间点、批处理过程的批次、生物个体或DOE协议试验的测量。在进行任何机器学习任务之前，通常在初步数据分析中使用PCA。\n",
        "\n",
        "### PCA的工作原理\n",
        "\n",
        "1. **考虑矩阵**：设有一个矩阵$X$，它有$N$行（也称为“观测值”）和$K$列（也称为“变量”）。\n",
        "\n",
        "2. **变量空间构造**：对于这个矩阵，我们构造一个与变量数量相同维度的变量空间。每个变量代表一个坐标轴。根据缩放标准（通常是缩放到单位方差），对每个变量的长度进行了标准化。\n",
        "\n",
        "3. **数据点定位**：接下来，将$X$矩阵的每一行（观测值）放置在$K$维变量空间中。因此，数据表中的行在这个空间中形成了一群点。\n",
        "\n",
        "4. **均值中心化**：均值中心化过程包括从数据中减去变量平均值。平均值向量对应于$K$空间中的一个点。通过减去平均值，相当于重新定位坐标系统，使得平均点现在是原点。\n",
        "\n",
        "### 主成分的计算\n",
        "\n",
        "- **第一主成分（PC1）**：在均值中心化和缩放到单位方差后，数据集准备好计算第一个概要指标，即第一主成分（PC1）。这个组成部分是$K$维变量空间中最能以最小二乘法逼近数据的线。这条线通过平均点。每个观测值（黄点）现在可以投影到这条线上，以获得沿PC线的坐标值。这个新的坐标值也称为得分。\n",
        "\n",
        "- **第二主成分（PC2）**：通常，一个概要指标或主成分不足以模拟数据集的系统变化。因此，计算第二个概要指标——第二主成分（PC2）。第二PC也由$K$维变量空间中的一条线表示，这条线与第一PC正交。第二主成分（PC2）的方向被选择为反映数据中第二大变异源的同时与第一PC正交。PC2也通过平均点。\n",
        "\n",
        "### PC1和PC2的计算方法\n",
        "\n",
        "1. **标准化数据**：确保每个特征的均值为0，标准差为1。这对PCA很重要，因为它对变量的尺度敏感。\n",
        "\n",
        "2. **计算协方差矩阵**：协方差矩阵是一个方阵，捕获数据集中每对特征之间的协方差。协方差矩阵$\\Sigma_b$的计算公式为：\n",
        "   $$\n",
        "   \\Sigma_b = \\frac{1}{n-1} X^T X\n",
        "   $$\n",
        "   其中$X$是标准化后的数据矩阵（维度$n \\times k$），有$n$个观测值和$k$个特征。\n",
        "\n",
        "3. **计算协方差矩阵的特征值和特征向量**：特征向量表示数据集中最大方差的方向，特征值表示这些方向上方差的大小。\n",
        "\n",
        "- 特征向量${v_1, v_2, ..., v_k}$和特征值${\\lambda_1, \\lambda_2, ..., \\lambda_k}$满足以下关系：\n",
        "  $$\n",
        "  \\Sigma_b v_i = \\lambda_i v_i\n",
        "  $$\n",
        "- $\\Sigma_b$的特征分解可以表示为：\n",
        "  $$\n",
        "  \\Sigma_b = V \\Lambda V^T\n",
        "  $$\n",
        "  其中$V$是包含所有特征向量的矩阵，$\\Lambda$是一个对角矩阵，对角线上的元素是特征值。\n",
        "\n",
        "4. **按特征值降序排序特征向量**：与最大特征值相关联的特征向量是第一主成分（PC1），与第二大特征值相关联的特征向量是第二主成分（PC2）。\n",
        "\n",
        "- 因此，PC1和PC2可以表示为：\n",
        "  $$\n",
        "  \\text{PC1} = v_1\n",
        "  $$\n",
        "  $$\n",
        "  \\text{PC2} = v_2\n",
        "  $$\n",
        "  其中$v_1$和$v_2$分别是对应于最大和第二大特征值的特征向量。\n",
        "\n",
        "### 主成分分析的作用\n",
        "\n",
        "通过这种方式，PCA能够将数据从原始的特征空间转换到由主成分构成的新空间，这些主成分定义了数据中方差最大的方向。这不仅有助于降维，减少计算负担，而且还能揭示数据中潜在的模式和结构。\n",
        "\n",
        "- **数据压缩**：通过选择前几个主成分，我们可以用较少的变量来近似原始数据集，实现数据压缩同时保留最重要的信息。\n",
        "\n",
        "- **可视化**：特别是在二维或三维空间中使用前两个或三个主成分，可以帮助我们可视化并理解高维数据。\n",
        "\n",
        "- **去相关**：PCA通过对数据进行线性变换，将原始特征转换为彼此统计独立的主成分，有助于去除特征之间的相关性。\n",
        "\n",
        "PCA是一种强大的工具，广泛应用于探索性数据分析、预处理步骤以及复杂数据集中的模式识别等领域。"
      ],
      "metadata": {
        "id": "6l683Jd-2eZG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "当从数据中提取出两个主成分时，它们共同定义了一个平面，这个平面可以看作是$K$维变量空间的一个窗口。通过将所有观测值投影到这个低维子空间上，并绘制结果，我们可以可视化被研究数据集的结构。在这个平面上观测值的坐标值称为得分（Scores），因此这种投影配置的绘制被称为得分图（Score Plot）。\n",
        "\n",
        "# 什么是得分（Score）？\n",
        "\n",
        "- 主成分得分是通过将标准化的$X$与$X$的协方差的特征向量（载荷）相乘得到的。回忆一下，$V$是协方差$\\Sigma_b$的特征向量（载荷）矩阵。\n",
        "\n",
        "- 我们按照相应特征值的降序排列$V$的列，那么得分$T$可以计算为：\n",
        "  $$\n",
        "  T_{n \\times k} = XV\n",
        "  $$\n",
        "  其中，$T$的第一列包含第一主成分（PC1）的得分，第二列包含第二主成分（PC2）的得分，依此类推。\n",
        "\n",
        "# 如何理解模型平面和得分图？\n",
        "\n",
        "- **模型平面**：通过两个主成分定义的模型平面提供了一个查看数据在新坐标系下的二维表示。这个平面最大程度地保留了原始数据的变异性，尽管它减少了数据的维度。\n",
        "\n",
        "- **得分图**：在得分图中，每个点代表一个观测值在主成分定义的新坐标系中的位置。得分图能够帮助我们理解数据点之间的相对位置和聚集情况，揭示潜在的模式或群组。\n",
        "\n",
        "通过利用主成分分析（PCA）中的得分和载荷，我们不仅能够降低数据的维度，还能够揭示数据中的内在结构，这对于数据探索和预处理尤其有用。例如，在生物信息学或市场细分研究中，得分图可以帮助识别相似的样本或客户群体，为进一步的分析和决策提供依据。"
      ],
      "metadata": {
        "id": "abJZoJ3a36aq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 标准PCA的局限性\n",
        "\n",
        "- **对异常值和噪声的敏感性**：在存在异常值或噪声的数据中，标准PCA可能会被这些异常值或噪声所影响，导致识别出的主成分并不能有效地反映数据的真实结构。\n",
        "\n",
        "# Robust PCA的需求\n",
        "\n",
        "为了克服标准PCA的局限性，开发了强健PCA（Robust PCA）。\n",
        "\n",
        "- **设计初衷**：与标准PCA不同，强健PCA旨在将数据的低秩结构（代表真实信号）与稀疏的噪声或异常值分离。这使得强健PCA更适用于现实世界中经常被噪声污染或包含异常值的数据集。\n",
        "\n",
        "### Robust PCA的数学表述\n",
        "\n",
        "- **目标**：将数据矩阵$X$分解为一个低秩矩阵$L$和一个稀疏矩阵$S$，使得$X = L + S$。\n",
        "\n",
        "- **优化问题**：通过解决以下优化问题来实现分解：\n",
        "  $$\n",
        "  \\min_{L,S} \\; ||L||_* + \\lambda ||S||_1\n",
        "  $$\n",
        "  满足约束条件$X = L + S$，其中$||L||_*$表示$L$的迹范数（Trace Norm），$||S||_1$表示$S$的$L_1$范数，$\\lambda$是正则化参数。\n",
        "\n",
        "- **迹范数（Trace Norm）**：矩阵$A$的迹范数定义为其奇异值（Singular Values，即特征值的平方根）之和。\n",
        "\n",
        "- **$L_1$范数**：矩阵$A$的$L_1$范数定义为其所有元素的绝对值之和。\n",
        "\n",
        "Robust PCA通过这种方式处理数据，使其能够在保留数据主要结构的同时，有效地削减或分离出异常值和噪声。这种方法特别适用于那些数据质量受到损害，或者含有大量异常值和噪声的情况，如图像处理、信号处理等领域中的应用。通过这种方式，强健PCA能够提供比标准PCA更准确、更可靠的数据降维和特征提取结果。"
      ],
      "metadata": {
        "id": "r2Ompp9f582d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clustering 聚类\n",
        "\n",
        "聚类是将整个数据根据数据中的模式划分为若干组（也称为簇）的过程。具体来说，我们有一个训练集$x^{(1)}, ..., x^{(m)}$，并希望将数据分组到几个紧密的“簇”中。这里，我们像往常一样为每个数据点$x^{(i)}$给出特征向量，但没有标签$y^{(i)}$（这使得聚类成为一个无监督学习问题）。我们的目标是预测$k$个质心和每个数据点的一个标签“$c^{(i)}$”（对应于不同的簇）。K-均值聚类算法如下：\n",
        "\n",
        "### K-均值（K-Means）\n",
        "\n",
        "K-均值是最流行的“聚类”算法之一。\n",
        "\n",
        "- **质心存储**：K-均值存储$k$个质心，用它们来定义簇。如果一个点更靠近某个簇的质心而不是其他质心，那么这个点被认为属于那个特定的簇。\n",
        "\n",
        "- **质心寻找**：K-均值通过交替执行以下两步来找到最佳质心：\n",
        "  1. 根据当前质心，将数据点分配到簇中。\n",
        "  2. 根据数据点当前的簇分配情况，选择质心（簇的中心点）。\n",
        "\n",
        "- **K的值**：K的值通常是预先指定的。\n",
        "\n",
        "### K-均值算法\n",
        "\n",
        "1. **初始化质心**：随机初始化$k$个质心$\\mu_1, \\mu_2, \\cdots, \\mu_k \\in \\mathbb{R}^n$。\n",
        "\n",
        "2. **迭代直至收敛**：\n",
        "   - 对每个$i$，设置$c^{(i)} = \\arg \\min_j ||x^{(i)} - \\mu_j||^2$，这意味着将每个数据点$x^{(i)}$分配给与其最近的质心$\\mu_j$所代表的簇。\n",
        "   - 对每个$j$，更新质心$\\mu_j$为分配给簇$j$的所有点$x^{(i)}$的平均值，即$\\mu_j = \\frac{\\sum_{i=1}^{m} 1\\{c^{(i)}=j\\}x^{(i)}}{\\sum_{i=1}^{m} 1\\{c^{(i)}=j\\}}$，其中$1\\{\\}$是指示函数，如果$c^{(i)}=j$为真则返回1，否则返回0。\n",
        "\n",
        "通过这种方式，K-均值算法能够将数据点有效地分组到不同的簇中，每个簇由一个质心代表。K-均值是一种强大的聚类工具，尤其适用于数据点自然聚集成不同簇的情况。然而，选择合适的K值和初始质心对算法的结果有很大影响，可能需要多次运行算法以找到最佳的聚类结果。"
      ],
      "metadata": {
        "id": "0hNKHFkU6Jkg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fuzzy C-Means clustering\n",
        "\n",
        "模糊C均值（Fuzzy C-Means, FCM）是一种聚类算法，它允许一个数据点属于两个或多个簇。这种方法经常用于模式识别，是传统的K-均值聚类算法的扩展。与K-均值不同，K-均值中每个数据点严格属于一个簇，FCM引入了成员等级的概念，允许数据点以不同程度属于多个簇。\n",
        "\n",
        "给定一个数据集$X = \\{x_1, x_2, \\cdots, x_n\\}$，由$n$个数据点组成，FCM的目标是将数据分割为$c$个模糊簇，其中$c$是预定义的簇的数量。\n",
        "\n",
        "每个数据点$x_i$在每个簇$j$中都有一个隶属度$u_{ij}$，$u_{ij}$是一个介于0和1之间的数，表示$x_i$属于簇$j$的程度。FCM算法旨在最小化以下目标函数：\n",
        "\n",
        "$$\n",
        "J(U, V) = \\sum_{i=1}^{n} \\sum_{j=1}^{c} u_{ij}^m \\|x_i - v_j\\|^2\n",
        "$$\n",
        "\n",
        "其中，\n",
        "\n",
        "- $U = [u_{ij}]$是隶属度矩阵\n",
        "- $V = \\{v_1, v_2, \\cdots, v_c\\}$是簇中心的集合\n",
        "- $m$是模糊参数，控制簇模糊的级别\n",
        "- $\\|x_i - v_j\\|$是数据点$x_i$和簇中心$v_j$之间的欧几里得距离\n",
        "\n",
        "算法通过以下更新规则迭代更新隶属度矩阵$U$和簇中心$V$，直到收敛：\n",
        "\n",
        "#### 隶属度更新：\n",
        "\n",
        "$$\n",
        "u_{ij} = \\frac{1}{\\sum_{k=1}^{c} \\left( \\frac{\\|x_i - v_j\\|}{\\|x_i - v_k\\|} \\right)^{\\frac{2}{m-1}}}\n",
        "$$\n",
        "\n",
        "#### 簇中心更新：\n",
        "\n",
        "$$\n",
        "v_j = \\frac{\\sum_{i=1}^{n} u_{ij}^m x_i}{\\sum_{i=1}^{n} u_{ij}^m}\n",
        "$$\n",
        "\n",
        "当连续迭代之间的隶属度矩阵变化低于指定阈值时，算法停止。\n",
        "\n",
        "通过这种方式，FCM算法不仅能够将数据点灵活地分配到多个簇中，还能够处理数据中的不确定性和模糊性，使其在处理具有模糊边界或重叠区域的复杂数据集时特别有用。\n",
        "\n",
        "### 优势\n",
        "\n",
        "1. **对重叠数据集的高效处理**：FCM在处理有重叠的数据集时能够给出最佳结果，与K-均值算法相比，FCM在这类情况下通常表现得更好。这是因为FCM能够捕捉数据点之间的模糊边界，使得每个数据点不仅仅被硬性地分配到一个簇中。\n",
        "\n",
        "2. **柔性隶属度分配**：与K-均值不同，在K-均值中数据点必须严格属于一个簇中心，在FCM中，数据点被赋予每个簇中心的隶属度，这意味着数据点可以同时属于多个簇中心。这种柔性的隶属度分配允许FCM更灵活地表达数据点之间的关系和归属。\n",
        "\n",
        "### 劣势\n",
        "\n",
        "1. **簇数量的先验指定**：FCM需要事先指定簇的数量，这与K-均值算法类似。在实际应用中，确定最佳的簇数量往往是一个挑战，可能需要依赖领域知识或通过尝试不同的簇数量来确定。\n",
        "\n",
        "2. **欧几里得距离的不等权重问题**：FCM使用欧几里得距离来衡量数据点与簇中心之间的距离。然而，欧几里得距离可能会不等权重地影响底层因素，尤其是在特征具有不同尺度或类型的数据集中。在这些情况下，可能需要对数据进行适当的预处理，或者考虑使用其他距离度量。\n",
        "\n",
        "尽管存在上述劣势，FCM由于其灵活性和对数据重叠区域处理的有效性，在许多领域内仍然是一个非常有用的聚类工具。为了克服其劣势，研究人员和实践者可以探索使用其他类型的距离度量、自动选择簇数量的方法，或者将FCM与其他算法结合使用以获得更好的聚类效果。"
      ],
      "metadata": {
        "id": "TfhcUjPo7dZ5"
      }
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64102697",
   "metadata": {},
   "source": [
    "# 0.) Import the Credit Card Fraud Data From CCLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9582ffa9",
   "metadata": {},
   "source": [
    "上述代码展现了一个用于欺诈检测的机器学习流程，主要涉及数据预处理、特征工程、模型训练、处理数据不平衡问题、模型评估和结果分析等环节。以下是完整流程的思路和原理概述：\n",
    "\n",
    "### 数据预处理和特征工程\n",
    "\n",
    "1. **数据加载**：\n",
    "   - 使用`pandas`的`read_csv`函数从\"fraudTest.csv\"文件中加载数据。\n",
    "\n",
    "2. **选择相关特征**：\n",
    "   - 从数据集中选择与任务相关的特征，包括交易时间、交易类别、金额、城市人口和欺诈标记（目标变量）。\n",
    "\n",
    "3. **日期时间转换**：\n",
    "   - 将交易时间从字符串格式转换为`datetime`格式，以便提取时间相关的特征。\n",
    "\n",
    "4. **时间特征提取**：\n",
    "   - 从交易时间中提取具体的时间特征（如秒），这可能与欺诈行为有相关性。\n",
    "\n",
    "5. **独热编码**：\n",
    "   - 对分类特征（如交易类别）进行独热编码，转换为模型可处理的数值型特征。\n",
    "\n",
    "### 数据不平衡处理\n",
    "\n",
    "6. **重采样策略**：\n",
    "   - 采用过采样（增加少数类样本）、欠采样（减少多数类样本）和SMOTE（合成少数类过采样技术）等方法处理数据不平衡问题。\n",
    "\n",
    "### 模型训练和评估\n",
    "\n",
    "7. **训练逻辑回归模型**：\n",
    "   - 分别使用经过不同重采样处理后的数据训练逻辑回归模型。\n",
    "\n",
    "8. **模型性能评估**：\n",
    "   - 使用测试集对每个模型进行评估，并通过计算准确率等指标衡量模型性能。\n",
    "\n",
    "9. **混淆矩阵和敏感性计算**：\n",
    "   - 对每个模型的预测结果计算混淆矩阵，并进一步计算敏感性（召回率），评估模型对少数类（欺诈行为）的识别能力。\n",
    "\n",
    "### 性能指标计算和模型比较\n",
    "\n",
    "10. **性能指标计算函数**：\n",
    "    - 定义`calc_perf_metric`函数来计算多个性能指标，包括敏感性、特异性、精确度、召回率和F1分数，以全面评估模型性能。\n",
    "\n",
    "11. **遍历模型训练和评估**：\n",
    "    - 遍历不同的重采样方法和模型配置，训练模型并计算上述性能指标，将结果存储并汇总，便于比较不同策略的效果。\n",
    "\n",
    "### 总结\n",
    "\n",
    "整个流程展示了如何处理机器学习中的一个常见问题——数据不平衡，并探索了不同数据处理策略和模型配置对欺诈检测任务性能的影响。通过对每个模型的混淆矩阵和关键性能指标的详细分析，提供了一个全面的评估视角，有助于选择最适合解决特定问题的模型和策略。此外，通过系统地遍历不同配置和评估方法，这个流程为处理类似问题提供了一个结构化和可重复的框架。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "103ed515",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "499b259a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"fraudTest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c3caa91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>trans_date_trans_time</th>\n",
       "      <th>cc_num</th>\n",
       "      <th>merchant</th>\n",
       "      <th>category</th>\n",
       "      <th>amt</th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>gender</th>\n",
       "      <th>street</th>\n",
       "      <th>...</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>city_pop</th>\n",
       "      <th>job</th>\n",
       "      <th>dob</th>\n",
       "      <th>trans_num</th>\n",
       "      <th>unix_time</th>\n",
       "      <th>merch_lat</th>\n",
       "      <th>merch_long</th>\n",
       "      <th>is_fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-06-21 12:14:25</td>\n",
       "      <td>2291163933867244</td>\n",
       "      <td>fraud_Kirlin and Sons</td>\n",
       "      <td>personal_care</td>\n",
       "      <td>2.86</td>\n",
       "      <td>Jeff</td>\n",
       "      <td>Elliott</td>\n",
       "      <td>M</td>\n",
       "      <td>351 Darlene Green</td>\n",
       "      <td>...</td>\n",
       "      <td>33.9659</td>\n",
       "      <td>-80.9355</td>\n",
       "      <td>333497</td>\n",
       "      <td>Mechanical engineer</td>\n",
       "      <td>1968-03-19</td>\n",
       "      <td>2da90c7d74bd46a0caf3777415b3ebd3</td>\n",
       "      <td>1371816865</td>\n",
       "      <td>33.986391</td>\n",
       "      <td>-81.200714</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-06-21 12:14:33</td>\n",
       "      <td>3573030041201292</td>\n",
       "      <td>fraud_Sporer-Keebler</td>\n",
       "      <td>personal_care</td>\n",
       "      <td>29.84</td>\n",
       "      <td>Joanne</td>\n",
       "      <td>Williams</td>\n",
       "      <td>F</td>\n",
       "      <td>3638 Marsh Union</td>\n",
       "      <td>...</td>\n",
       "      <td>40.3207</td>\n",
       "      <td>-110.4360</td>\n",
       "      <td>302</td>\n",
       "      <td>Sales professional, IT</td>\n",
       "      <td>1990-01-17</td>\n",
       "      <td>324cc204407e99f51b0d6ca0055005e7</td>\n",
       "      <td>1371816873</td>\n",
       "      <td>39.450498</td>\n",
       "      <td>-109.960431</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-06-21 12:14:53</td>\n",
       "      <td>3598215285024754</td>\n",
       "      <td>fraud_Swaniawski, Nitzsche and Welch</td>\n",
       "      <td>health_fitness</td>\n",
       "      <td>41.28</td>\n",
       "      <td>Ashley</td>\n",
       "      <td>Lopez</td>\n",
       "      <td>F</td>\n",
       "      <td>9333 Valentine Point</td>\n",
       "      <td>...</td>\n",
       "      <td>40.6729</td>\n",
       "      <td>-73.5365</td>\n",
       "      <td>34496</td>\n",
       "      <td>Librarian, public</td>\n",
       "      <td>1970-10-21</td>\n",
       "      <td>c81755dbbbea9d5c77f094348a7579be</td>\n",
       "      <td>1371816893</td>\n",
       "      <td>40.495810</td>\n",
       "      <td>-74.196111</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2020-06-21 12:15:15</td>\n",
       "      <td>3591919803438423</td>\n",
       "      <td>fraud_Haley Group</td>\n",
       "      <td>misc_pos</td>\n",
       "      <td>60.05</td>\n",
       "      <td>Brian</td>\n",
       "      <td>Williams</td>\n",
       "      <td>M</td>\n",
       "      <td>32941 Krystal Mill Apt. 552</td>\n",
       "      <td>...</td>\n",
       "      <td>28.5697</td>\n",
       "      <td>-80.8191</td>\n",
       "      <td>54767</td>\n",
       "      <td>Set designer</td>\n",
       "      <td>1987-07-25</td>\n",
       "      <td>2159175b9efe66dc301f149d3d5abf8c</td>\n",
       "      <td>1371816915</td>\n",
       "      <td>28.812398</td>\n",
       "      <td>-80.883061</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2020-06-21 12:15:17</td>\n",
       "      <td>3526826139003047</td>\n",
       "      <td>fraud_Johnston-Casper</td>\n",
       "      <td>travel</td>\n",
       "      <td>3.19</td>\n",
       "      <td>Nathan</td>\n",
       "      <td>Massey</td>\n",
       "      <td>M</td>\n",
       "      <td>5783 Evan Roads Apt. 465</td>\n",
       "      <td>...</td>\n",
       "      <td>44.2529</td>\n",
       "      <td>-85.0170</td>\n",
       "      <td>1126</td>\n",
       "      <td>Furniture designer</td>\n",
       "      <td>1955-07-06</td>\n",
       "      <td>57ff021bd3f328f8738bb535c302a31b</td>\n",
       "      <td>1371816917</td>\n",
       "      <td>44.959148</td>\n",
       "      <td>-85.884734</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 trans_date_trans_time            cc_num  \\\n",
       "0           0   2020-06-21 12:14:25  2291163933867244   \n",
       "1           1   2020-06-21 12:14:33  3573030041201292   \n",
       "2           2   2020-06-21 12:14:53  3598215285024754   \n",
       "3           3   2020-06-21 12:15:15  3591919803438423   \n",
       "4           4   2020-06-21 12:15:17  3526826139003047   \n",
       "\n",
       "                               merchant        category    amt   first  \\\n",
       "0                 fraud_Kirlin and Sons   personal_care   2.86    Jeff   \n",
       "1                  fraud_Sporer-Keebler   personal_care  29.84  Joanne   \n",
       "2  fraud_Swaniawski, Nitzsche and Welch  health_fitness  41.28  Ashley   \n",
       "3                     fraud_Haley Group        misc_pos  60.05   Brian   \n",
       "4                 fraud_Johnston-Casper          travel   3.19  Nathan   \n",
       "\n",
       "       last gender                       street  ...      lat      long  \\\n",
       "0   Elliott      M            351 Darlene Green  ...  33.9659  -80.9355   \n",
       "1  Williams      F             3638 Marsh Union  ...  40.3207 -110.4360   \n",
       "2     Lopez      F         9333 Valentine Point  ...  40.6729  -73.5365   \n",
       "3  Williams      M  32941 Krystal Mill Apt. 552  ...  28.5697  -80.8191   \n",
       "4    Massey      M     5783 Evan Roads Apt. 465  ...  44.2529  -85.0170   \n",
       "\n",
       "   city_pop                     job         dob  \\\n",
       "0    333497     Mechanical engineer  1968-03-19   \n",
       "1       302  Sales professional, IT  1990-01-17   \n",
       "2     34496       Librarian, public  1970-10-21   \n",
       "3     54767            Set designer  1987-07-25   \n",
       "4      1126      Furniture designer  1955-07-06   \n",
       "\n",
       "                          trans_num   unix_time  merch_lat  merch_long  \\\n",
       "0  2da90c7d74bd46a0caf3777415b3ebd3  1371816865  33.986391  -81.200714   \n",
       "1  324cc204407e99f51b0d6ca0055005e7  1371816873  39.450498 -109.960431   \n",
       "2  c81755dbbbea9d5c77f094348a7579be  1371816893  40.495810  -74.196111   \n",
       "3  2159175b9efe66dc301f149d3d5abf8c  1371816915  28.812398  -80.883061   \n",
       "4  57ff021bd3f328f8738bb535c302a31b  1371816917  44.959148  -85.884734   \n",
       "\n",
       "   is_fraud  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17d159c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从原始DataFrame中选择与分析相关的列，并创建df_select DataFrame作为分析的基础。\n",
    "df_select = df[[\"trans_date_trans_time\", \"category\", \"amt\", \"city_pop\", \"is_fraud\"]].copy()\n",
    "\n",
    "# 将\"trans_date_trans_time\"列中的字符串转换为datetime格式，便于后续的时间相关分析\n",
    "df_select[\"trans_date_trans_time\"] = pd.to_datetime(df_select[\"trans_date_trans_time\"])\n",
    "\n",
    "# 从\"trans_date_trans_time\"列提取秒作为新的特征\"time_var\"，可能用于分析交易时间和欺诈行为的关系。\n",
    "df_select[\"time_var\"] = df_select[\"trans_date_trans_time\"].dt.second\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9695b19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征工程\n",
    "\n",
    "# 对\"category\"列进行独热编码，并丢弃原始的时间列和目标变量列，得到用于训练的特征集X\n",
    "X = pd.get_dummies(df_select, columns=[\"category\"]).drop([\"trans_date_trans_time\", \"is_fraud\"], axis=1)\n",
    "\n",
    "# 从df_select中提取\"is_fraud\"列作为目标变量y\n",
    "y = df_select[\"is_fraud\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7591162b",
   "metadata": {},
   "source": [
    "# 1.) Use scikit learn preprocessing to split the data into 70/30 in out of sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9456777",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2b04114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据划分为训练集和测试集，测试集占总数据的30%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dee222b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进一步将测试集划分为最终的测试集和保留集，各占原测试集的50%\n",
    "X_test, X_holdout, y_test, y_holdout = train_test_split(X_test, y_test, test_size = .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d184432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用StandardScaler对特征进行标准化，以确保模型不会因特征的量纲不同而受到影响\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_holdout = scaler.transform(X_holdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ea7e59",
   "metadata": {},
   "source": [
    "# 2.) Make three sets of training data (Oversample, Undersample and SMOTE)¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f58debc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a49b086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理数据不平衡\n",
    "\n",
    "# RandomOverSampler通过随机复制少数类的样本来增加其数量，直到与多数类的样本数量相等，从而达到类别平衡的效果\n",
    "# 这种方法简单直接，但可能导致过拟合，因为它只是简单地重复少数类样本。\n",
    "ros = RandomOverSampler()\n",
    "over_X, over_y = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# RandomUnderSampler通过随机减少多数类的样本数量来匹配少数类的样本数量，从而实现类别平衡。\n",
    "# 这种方法减少了数据集的大小，可能会导致信息丢失，但可以提高模型对少数类的关注。\n",
    "rus = RandomUnderSampler()\n",
    "under_X, under_y = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# SMOTE（Synthetic Minority Over-sampling Technique）通过在少数类样本之间插值生成新的合成样本来增加少数类的样本数量。\n",
    "# 与简单的过采样不同，SMOTE通过生成新样本增加了类别的多样性，有助于避免过拟合。\n",
    "smote = SMOTE()\n",
    "smote_X, smote_y = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98343dbf",
   "metadata": {},
   "source": [
    "# 3.) Train three logistic regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f71acd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8796b5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练逻辑回归模型\n",
    "# 分别使用经过RandomOverSampler、RandomUnderSampler和SMOTE处理后的数据集训练逻辑回归模型。\n",
    "# 每种处理方法都可能对模型性能产生不同的影响。\n",
    "\n",
    "over_log = LogisticRegression().fit(over_X, over_y)\n",
    "\n",
    "under_log = LogisticRegression().fit(under_X, under_y)\n",
    "\n",
    "smote_log = LogisticRegression().fit(smote_X, smote_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef94fa20",
   "metadata": {},
   "source": [
    "# 4.) Test the three models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf34907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每个处理过的数据集训练出的模型都在测试集上进行评估，计算准确率来衡量模型性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a30fab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9186160896374673"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "over_log.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01765c13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9240744739557091"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "under_log.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a750f712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9154850164351352"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smote_log.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415ae86e",
   "metadata": {},
   "source": [
    "We see SMOTE performing with higher accuracy but is ACCURACY really the best measure?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875b52f5",
   "metadata": {},
   "source": [
    "# 5.) Which performed best in Out of Sample metrics?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad8e0c9",
   "metadata": {},
   "source": [
    "在解释混淆矩阵和敏感性计算的代码之前先确保我们理解混淆矩阵的四个基本组成部分：\n",
    "\n",
    "- **真正例（TP, True Positives）**：模型正确预测正类的数量。\n",
    "- **假正例（FP, False Positives）**：模型错误地将负类预测为正类的数量。\n",
    "- **真负例（TN, True Negatives）**：模型正确预测负类的数量。\n",
    "- **假负例（FN, False Negatives）**：模型错误地将正类预测为负类的数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3dcf6f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "593b1249",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f63fecf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[76363,  6696],\n",
       "       [   88,   211]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对于使用过采样处理的模型，代码中使用测试集数据进行预测并计算混淆矩阵\n",
    "\n",
    "y_pred = over_log.predict(X_test) # over_log.predict(X_test)是对测试集X_test的预测结果\n",
    "cm = confusion_matrix(y_true, y_pred) # confusion_matrix函数用于计算真实标签y_true（即y_test）和预测标签y_pred之间的混淆矩阵\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "693c2e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over Sample Sensitivity :  0.705685618729097\n"
     ]
    }
   ],
   "source": [
    "# 敏感性（或召回率）计算\n",
    "\n",
    "# 使用混淆矩阵的第二行（索引为1，对应于正类），即cm[1,1] / (cm[1,0] + cm[1,1])\n",
    "# cm[1,1]是真正例（TP）的数量，cm[1,0]是假负例（FN）的数量\n",
    "# 敏感性衡量了模型识别正类样本的能力\n",
    "print(\"Over Sample Sensitivity : \", cm[1,1] /( cm[1,0] + cm[1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4e55068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[76820,  6239],\n",
       "       [   90,   209]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = under_log.predict(X_test)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc9cf5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under Sample Sensitivity :  0.6989966555183946\n"
     ]
    }
   ],
   "source": [
    "print(\"Under Sample Sensitivity : \", cm[1,1] /( cm[1,0] + cm[1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bccd091d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[76102,  6957],\n",
       "       [   88,   211]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = smote_log.predict(X_test)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "471984b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE Sample Sensitivity :  0.705685618729097\n"
     ]
    }
   ],
   "source": [
    "print(\"SMOTE Sample Sensitivity : \", cm[1,1] /( cm[1,0] + cm[1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c920847",
   "metadata": {},
   "source": [
    "# 7.) We want to compare oversampling, Undersampling and SMOTE across our 3 models (Logistic Regression, Logistic Regression Lasso and Decision Trees).\n",
    "\n",
    "# Make a dataframe that has a dual index and 9 Rows.\n",
    "# Calculate: Sensitivity, Specificity, Precision, Recall and F1 score. for out of sample data.\n",
    "# Notice any patterns across perfomance for this model. Does one totally out perform the others IE. over/under/smote or does a model perform better DT, Lasso, LR?\n",
    "# Choose what you think is the best model and why. test on Holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f869d5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01c28bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 扩展模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "484a5c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对于重采样方法和模型配置，代码定义了两个字典来提供多样化的选择\n",
    "# 这些字典分别包含了不同的重采样策略和多个模型配置，以便进行扩展训练和评估\n",
    "\n",
    "resampling_methods = {\n",
    "    \"over\": RandomOverSampler(),\n",
    "    \"under\": RandomUnderSampler(),\n",
    "    \"smote\": SMOTE()\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"LOG\":LogisticRegression(),\n",
    "    \"LASSO\": LogisticRegression(penalty = \"l1\", C = 2., solver = \"liblinear\"),\n",
    "    \"DTREE\": DecisionTreeClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a80ac95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 性能指标的计算通过自定义函数calc_perf_metric实现\n",
    "# 这个函数使用confusion_matrix和其他sklearn.metrics函数来计算敏感性、特异性、精确度、召回率和F1分数，提供了模型性能的全面视角。\n",
    "\n",
    "def calc_perf_metric(y_true, y_pred):\n",
    "    tn,fp,fn,tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    precision = precision_score(y_true, y_pred),\n",
    "    recall = recall_score(y_true, y_pred),\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    \n",
    "    return(sensitivity, specificity, precision, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47df2e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_models = {}\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "133ea6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 遍历训练和评估\n",
    "\n",
    "# 遍历了所有重采样方法和模型配置的组合，对每个组合进行了训练和性能评估，并将评估结果存储在results列表中。\n",
    "# 最终，这些结果被汇总到result_df DataFrame中，便于后续分析和比较\n",
    "\n",
    "for resample_key, resampler in resampling_methods.items():\n",
    "    resample_X, resample_y = resampler.fit_resample(X_train, y_train)\n",
    "    \n",
    "    for model_key, model in model_configs.items():\n",
    "        combined_key = f\"{resample_key}_{model_key}\"\n",
    "        \n",
    "        m = model.fit(resample_X, resample_y)\n",
    "        \n",
    "        trained_models[combined_key] = m\n",
    "        \n",
    "        y_pred = m.predict(X_test)\n",
    "        \n",
    "        sensitivity, specificity, precision, recall, f1 = calc_perf_metric(y_test,y_pred)\n",
    "        \n",
    "        results.append({\"Model\": combined_key,\n",
    "                       \"Sensitivity\" : sensitivity,\n",
    "                       \"Specificity\": specificity,\n",
    "                       \"Precision\": precision,\n",
    "                       \"Recall\": recall,\n",
    "                       \"F1\" : f1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0dbdda3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "15019a2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>over_LOG</td>\n",
       "      <td>0.705686</td>\n",
       "      <td>0.915205</td>\n",
       "      <td>(0.02908740005514199,)</td>\n",
       "      <td>(0.705685618729097,)</td>\n",
       "      <td>0.055872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>over_LASSO</td>\n",
       "      <td>0.705686</td>\n",
       "      <td>0.915277</td>\n",
       "      <td>(0.02911147902869757,)</td>\n",
       "      <td>(0.705685618729097,)</td>\n",
       "      <td>0.055916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>over_DTREE</td>\n",
       "      <td>0.515050</td>\n",
       "      <td>0.998507</td>\n",
       "      <td>(0.5539568345323741,)</td>\n",
       "      <td>(0.5150501672240803,)</td>\n",
       "      <td>0.533795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>under_LOG</td>\n",
       "      <td>0.705686</td>\n",
       "      <td>0.926751</td>\n",
       "      <td>(0.0335186656076251,)</td>\n",
       "      <td>(0.705685618729097,)</td>\n",
       "      <td>0.063998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>under_LASSO</td>\n",
       "      <td>0.705686</td>\n",
       "      <td>0.926257</td>\n",
       "      <td>(0.03330176767676768,)</td>\n",
       "      <td>(0.705685618729097,)</td>\n",
       "      <td>0.063602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>under_DTREE</td>\n",
       "      <td>0.946488</td>\n",
       "      <td>0.949265</td>\n",
       "      <td>(0.06293084278407828,)</td>\n",
       "      <td>(0.9464882943143813,)</td>\n",
       "      <td>0.118015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>smote_LOG</td>\n",
       "      <td>0.705686</td>\n",
       "      <td>0.914579</td>\n",
       "      <td>(0.028880372296742403,)</td>\n",
       "      <td>(0.705685618729097,)</td>\n",
       "      <td>0.055490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>smote_LASSO</td>\n",
       "      <td>0.705686</td>\n",
       "      <td>0.914663</td>\n",
       "      <td>(0.028908069598575146,)</td>\n",
       "      <td>(0.705685618729097,)</td>\n",
       "      <td>0.055541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>smote_DTREE</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.993198</td>\n",
       "      <td>(0.26813471502590674,)</td>\n",
       "      <td>(0.6923076923076923,)</td>\n",
       "      <td>0.386555</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Model  Sensitivity  Specificity                Precision  \\\n",
       "0     over_LOG     0.705686     0.915205   (0.02908740005514199,)   \n",
       "1   over_LASSO     0.705686     0.915277   (0.02911147902869757,)   \n",
       "2   over_DTREE     0.515050     0.998507    (0.5539568345323741,)   \n",
       "3    under_LOG     0.705686     0.926751    (0.0335186656076251,)   \n",
       "4  under_LASSO     0.705686     0.926257   (0.03330176767676768,)   \n",
       "5  under_DTREE     0.946488     0.949265   (0.06293084278407828,)   \n",
       "6    smote_LOG     0.705686     0.914579  (0.028880372296742403,)   \n",
       "7  smote_LASSO     0.705686     0.914663  (0.028908069598575146,)   \n",
       "8  smote_DTREE     0.692308     0.993198   (0.26813471502590674,)   \n",
       "\n",
       "                  Recall        F1  \n",
       "0   (0.705685618729097,)  0.055872  \n",
       "1   (0.705685618729097,)  0.055916  \n",
       "2  (0.5150501672240803,)  0.533795  \n",
       "3   (0.705685618729097,)  0.063998  \n",
       "4   (0.705685618729097,)  0.063602  \n",
       "5  (0.9464882943143813,)  0.118015  \n",
       "6   (0.705685618729097,)  0.055490  \n",
       "7   (0.705685618729097,)  0.055541  \n",
       "8  (0.6923076923076923,)  0.386555  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

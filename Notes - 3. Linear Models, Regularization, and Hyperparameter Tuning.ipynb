{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Linear Models, Regularization, and Hyperparameter Tuning\n",
        "\n",
        "1. **线性模型（Linear Models）：**\n",
        "   - 线性模型是一类重要的机器学习模型，用于回归和分类任务。它们的核心思想是使用一个或多个自变量的线性组合来预测目标变量的值。典型的线性模型包括线性回归、逻辑回归等。\n",
        "   - 线性模型因其简单性、易于理解和实现而广受欢迎。它们通常是解决问题的第一步尝试，尤其是当我们认为数据之间存在线性关系时。\n",
        "\n",
        "2. **正则化（Regularization）：**\n",
        "   - 正则化是一种用于减少模型过拟合的技术，增加模型的泛化能力。通过在损失函数中添加一个正则化项（如L1范数、L2范数），可以约束模型的复杂度，避免模型在训练数据上过于完美拟合，而无法泛化到未见过的数据上。\n",
        "   - 正则化对于处理具有多重共线性（特征间高度相关）的数据集、当数据特征数多于样本数时尤其重要，它能够帮助减少模型的复杂度，防止模型过于依赖训练数据的噪声。\n",
        "\n",
        "3. **超参数调优（Hyperparameter Tuning）：**\n",
        "   - 超参数是在开始学习过程之前设置的参数，与模型训练过程中优化的参数（如权重）不同，超参数不会在学习过程中改变。超参数调优是选择一组最优超参数，使模型能够提供最佳性能的过程。\n",
        "   - 超参数调优技术包括网格搜索、随机搜索、贝叶斯优化等。这一过程对于提高模型性能、使模型适应各种数据集至关重要。\n",
        "\n",
        "综合来看，该章节涉及的是使用线性模型进行机器学习任务时，如何通过正则化技术防止过拟合，并通过超参数调优来进一步提升模型性能的一系列方法和实践。"
      ],
      "metadata": {
        "id": "x3ZGu1rXA1dY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 处理Multicollinearity - Shrinkage Methods收缩方法\n",
        "\n",
        "## 收缩方法 I：岭回归（Ridge Regression）\n",
        "\n",
        "岭回归是一种使用$L2$范数惩罚项的线性回归形式，其惩罚项为 $\\|\\beta\\|^2 = \\sum_{j=1}^p \\beta_j^2 = \\beta^T \\beta$。岭回归的目标函数定义为：\n",
        "\n",
        "$$\n",
        "\\hat{\\beta}_{\\text{ridge}} = \\arg\\min_\\beta (y - X\\beta)^T(y - X\\beta) + \\lambda \\|\\beta\\|^2\n",
        "$$\n",
        "\n",
        "- 其中，$\\lambda \\|\\beta\\|^2$是一个收缩惩罚项，它将 $\\beta$ 的估计值向零压缩。\n",
        "- 调节参数 $\\lambda > 0$ 控制了回归拟合与系数收缩之间的权衡。\n",
        "- 当 $\\lambda = 0$ 时，岭回归就是普通的线性回归；当 $\\lambda \\rightarrow \\infty$ 时，所有的 $\\beta_i$ 估计值将为零。\n",
        "\n",
        "岭回归还有一个等价的形式：\n",
        "\n",
        "$$\n",
        "\\hat{\\beta}_{\\text{ridge}, \\lambda} = \\arg\\min_\\beta (y - X\\beta)^T(y - X\\beta) \\quad \\text{subject to} \\quad \\|\\beta\\|^2 \\leq s\n",
        "$$\n",
        "\n",
        "## 收缩方法 II：套索回归（Lasso）\n",
        "\n",
        "套索回归使用$L1$范数作为惩罚项，其惩罚项为 $\\|\\beta\\|_1 = \\sum_{j=1}^p |\\beta_j|$。套索回归的目标函数定义为：\n",
        "\n",
        "$$\n",
        "\\hat{\\beta}_{\\text{lasso}} = \\arg\\min_\\beta (y - X\\beta)^T(y - X\\beta) + \\lambda \\|\\beta\\|_1\n",
        "$$\n",
        "\n",
        "或等价地：\n",
        "\n",
        "$$\n",
        "\\hat{\\beta}_{\\text{lasso}} = \\arg\\min_\\beta (y - X\\beta)^T(y - X\\beta) \\quad \\text{subject to} \\quad \\|\\beta\\|_1 \\leq s\n",
        "$$\n",
        "\n",
        "- 与岭回归相比，套索回归没有显式解。相反，通常使用**二次规划（Quadratic Programming, QP）算法**来解决上述优化问题。\n",
        "- 套索回归的一个重要特性是它可以产生稀疏解，即模型中许多系数可以被精确地估计为零。这一特性使得套索回归不仅有助于减少过拟合，而且可以通过仅保留对预测变量有重大贡献的特征来提高模型的解释性。\n",
        "\n",
        "通过使用这些收缩方法，我们可以有效地处理线性模型中的多重共线性问题，防止模型过拟合，并增强模型的可解释性，使其在预测失业率等经济指标时更为有效和可靠。\n"
      ],
      "metadata": {
        "id": "-59VqAO5BAfU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 自由度 ($df(\\lambda)$)\n",
        "\n",
        "“自由度”指的是在统计模型中用于估计参数的独立信息的数量。在没有正则化的标准线性模型中，自由度等于模型中独立参数的数量。\n",
        "\n",
        "当我们引入正则化时，如岭回归或套索回归，正则化参数 $\\lambda$ 会对模型的自由度产生影响。岭回归使用 L2 正则化，而套索回归使用 L1 正则化。在这两种方法中，$\\lambda$ 的作用是控制正则化的强度，通过惩罚项限制模型参数的大小。\n",
        "\n",
        "### 模型自由度的计算\n",
        "\n",
        "在岭回归中，模型的有效自由度 $df(\\lambda)$ 与 $\\lambda$ 的关系可以通过以下公式计算：\n",
        "\n",
        "$$\n",
        "df(\\lambda) = \\text{trace}(S) = \\text{trace}[X(X^T X + \\lambda I)^{-1}X^T]\n",
        "$$\n",
        "\n",
        "其中，$S$ 是收缩矩阵，$X$ 是设计矩阵，$X^T X$ 是设计矩阵的转置乘以它自己，$I$ 是单位矩阵，$\\lambda$ 是正则化参数。\n",
        "\n",
        "随着 $\\lambda$ 增大，每个预测变量对响应变量的影响被降低，从而减少了模型可以自由调整的参数数量，即降低了有效的自由度。\n",
        "\n",
        "### 自由度对模型影响\n",
        "\n",
        "在模型中，自由度越高意味着模型可以更自由地拟合数据，但这也可能导致过拟合，特别是当数据集中的噪音被模型当作信号来拟合时。相反，自由度较低的模型可能不够灵活去捕捉数据中的真实关系，但它们更不容易过拟合。因此，选择一个适当的 $\\lambda$ 值，以平衡模型复杂性和过拟合的风险，对于构建有效的预测模型至关重要。"
      ],
      "metadata": {
        "id": "q3yxwTWIXLI4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 收缩因子Shrinkage factor s\n",
        "\n",
        "在解释正则化参数 $\\lambda$ 的影响时，我们引入了一个称为“收缩因子” $s$ 的概念。这个因子有助于我们直观地理解正则化程度对模型的影响。以下是对收缩因子 $s$ 的详细解释，以及它是如何通过正则化参数 $\\lambda$ 定义的。\n",
        "\n",
        "收缩因子 $s$ 是与正则化参数 $\\lambda$ 相关联的一个度量，它表征了正则化的强度。\n",
        "\n",
        "$$\n",
        "s = \\frac{1}{1 + \\lambda}\n",
        "$$\n",
        "\n",
        "这个公式表明，收缩因子 $s$ 的值是 $\\lambda$ 的函数，它决定了正则化的程度。\n",
        "\n",
        "### 收缩因子对模型的影响\n",
        "\n",
        "- 当 $\\lambda$ 接近无穷大时，收缩因子 $s$ 接近 0。这意味着正则化强度最大，模型系数将被压缩到接近于零的程度，从而极大地限制了每个特征的影响力。\n",
        "- 相反，当 $\\lambda$ 接近 0 时，收缩因子 $s$ 接近 1。这表明正则化的影响最小，模型将接近普通最小二乘回归的解，也就是说，特征系数将接近于未经正则化时的估计。\n",
        "\n",
        "### 实际应用\n",
        "\n",
        "在前列腺癌预测模型的案例中，收缩因子 $s$ 可以帮助我们确定正则化参数 $\\lambda$ 的最佳值。通过观察不同的 $\\lambda$ 值对模型系数的具体影响，我们可以选择一个 $\\lambda$，使得模型既不会过于复杂（以避免过拟合），也不会过于简单（以捕捉数据中的关键信号）。\n",
        "\n",
        "收缩因子 $s$ 是调整模型复杂性的一个重要工具。选择合适的 $s$（即 $\\lambda$）对于获得一个泛化能力强、预测性能好的模型至关重要。在实际操作中，我们可以通过交叉验证等方法来选择一个最佳的 $\\lambda$ 值，从而确定一个合适的收缩因子 $s$，使得模型达到最优的预测效果。"
      ],
      "metadata": {
        "id": "gA9fUPIeXS9X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ridge vs Lasso\n",
        "\n",
        "正则化回归方法主要包括岭回归（ridge regression）和套索回归（lasso regression），它们通过在损失函数中加入一个额外的正则化项来收缩模型系数。下面将详细介绍这两种方法对系数的影响，以及它们如何影响模型的解释性和预测准确性。\n",
        "\n",
        "\n",
        "### 岭回归\n",
        "\n",
        "在岭回归中，正则化项是系数的平方和，即 L2 范数。这导致所有的系数都被一定程度上缩小，但不会变成稀疏的，即系数不会被收缩到零。\n",
        "\n",
        "$$\n",
        "\\text{Ridge Regularized Term: } \\lambda \\sum_{j=1}^p \\beta_j^2\n",
        "$$\n",
        "\n",
        "这种收缩会引入一些偏差，但通过缩小系数，可以降低模型的方差，提高模型的泛化能力。岭回归特别适合处理预测变量之间存在多重共线性的情况。\n",
        "\n",
        "### 套索回归\n",
        "\n",
        "与岭回归不同，套索回归的正则化项是系数的绝对值之和，即 L1 范数。这会使得一部分系数精确地缩小到零，从而产生一个稀疏模型。\n",
        "\n",
        "$$\n",
        "\\text{Lasso Regularized Term: } \\lambda \\sum_{j=1}^p |\\beta_j|\n",
        "$$\n",
        "\n",
        "套索回归通过产生稀疏解，提供了更高的可解释性。在模型中，只有一部分预测变量会被保留，而其它的系数会变成零。这使得套索回归特别有用于特征选择，帮助我们识别那些对于预测目标变量最有影响的预测变量。\n",
        "\n",
        "\n",
        "\n",
        "### 岭回归与套索回归的比较\n",
        "\n",
        "虽然岭回归和套索回归都会对系数进行收缩并引入偏差，但它们在模型解释性和预测准确性上可能会有不同的结果：\n",
        "\n",
        "- **岭回归**通常会包含所有的预测变量，其模型可能不够简洁，但在预测变量之间存在较强关联时能够提供更稳定的预测性能。\n",
        "- **套索回归**会产生一个更为简洁的模型，只包含一部分预测变量。这种方法在解释性方面更具优势，但当模型中包含了不相关的预测变量时，可能会牺牲一些预测准确性。\n",
        "\n",
        "总的来说，并没有一个统一的规则来判定哪一种回归方法更优。它们各有优势，适用于不同的数据集和问题。选择哪种方法取决于具体的应用场景，包括数据的特性、模型的目的以及对模型解释性的要求。在实际应用中，我们可以通过交叉验证来评估不同模型的预测准确性，以确定最适合的方法。在处理实际问题时，选择合适的正则化参数 $\\lambda$ 以及正则化方法（岭回归或套索回归）对于模型的性能和解释性都是至关重要的。"
      ],
      "metadata": {
        "id": "vQLHeDVzJPSj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 弹性网络（Elastic Net）\n",
        "\n",
        "\n",
        "弹性网络是一种正则化技术，它通过**结合L1正则化（套索回归）和L2正则化（岭回归）的惩罚项，形成了一个混合正则化的模型**。\n",
        "\n",
        "### 弹性网络的优势\n",
        "\n",
        "- **Lasso回归**：它通过L1正则化生成一个稀疏模型，有助于特征选择，但是当一组变量高度相关时，它只会选择其中的一个变量，而不是选择这一组变量。\n",
        "- **岭回归**：它通过L2正则化处理多重共线性问题，但不会产生一个稀疏模型。\n",
        "\n",
        "弹性网络旨在结合两种方法的优势，生成一个既能处理相关性强的预测变量，又能保持稀疏性的模型。\n",
        "\n",
        "### 弹性网络的惩罚项\n",
        "\n",
        "弹性网络引入了两个正则化参数 $\\lambda_1$ 和 $\\lambda_2$，分别控制L1和L2正则化的强度。其惩罚项的形式为：\n",
        "\n",
        "$$\n",
        "\\lambda_1 \\|\\beta\\|_1 + \\lambda_2 \\|\\beta\\|_2^2\n",
        "$$\n",
        "\n",
        "其中，$\\|\\beta\\|_1$ 表示系数向量 $\\beta$ 的L1范数（即系数的绝对值之和），而 $\\|\\beta\\|_2^2$ 表示系数向量 $\\beta$ 的L2范数的平方（即系数的平方和）。\n",
        "\n",
        "### 弹性网络的**超参数Hyperparameter**\n",
        "\n",
        "弹性网络还引入了一个额外的超参数 $\\alpha$，用于控制L1和L2正则化项的混合比例：\n",
        "\n",
        "$$\n",
        "\\text{Elastic Net Penalty: } \\alpha \\lambda \\|\\beta\\|_1 + \\frac{1-\\alpha}{2} \\lambda \\|\\beta\\|_2^2\n",
        "$$\n",
        "\n",
        "这里，$\\lambda$ 是控制整体正则化强度的参数，而 $\\alpha$ 是决定L1和L2之间混合比例的参数。$\\alpha = 1$ 时模型就是套索回归，而 $\\alpha = 0$ 时模型就是岭回归。\n",
        "\n",
        "### 弹性网络的应用\n",
        "\n",
        "由于结合了两种正则化的特点，弹性网络在处理具有多个相关预测变量的复杂数据集时尤其有用。它在实际应用中通常可以提供比单独使用L1或L2正则化更好的预测性能。\n",
        "\n",
        "选择最佳的 $\\lambda_1$、$\\lambda_2$ 和 $\\alpha$ 通常需要使用诸如交叉验证之类的技术来实现。\n",
        "\n",
        "总之，弹性网络提供了一种灵活的正则化框架，可以根据具体问题的需求调整正则化的强度和类型，以求得最佳的模型性能和解释性。\n"
      ],
      "metadata": {
        "id": "QeBBXm8LLuiC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 通用正则化：桥接估计量（Bridge Estimators）\n",
        "\n",
        "Bridge Estimators是一类通过惩罚项对回归系数进行正则化的模型。这些模型使用了一个形式为 $L_r(\\beta)$ 的正则化项，其中 $r$ 决定了正则化的类型和强度。\n",
        "\n",
        "\n",
        "给定一个预测变量矩阵 $X$ 和响应变量 $y$，桥接估计量的定义是：\n",
        "\n",
        "$$\n",
        "\\hat{\\beta}_{\\text{bridge}} = \\arg\\min_\\beta \\|y - X\\beta\\|^2 + \\lambda L_r(\\beta)\n",
        "$$\n",
        "\n",
        "其中，$\\|\\cdot\\|$ 表示欧氏距离，$\\lambda$ 是正则化参数，$L_r(\\beta)$ 是对系数 $\\beta$ 的正则化项。\n",
        "\n",
        "### 不同的正则化项 $L_r(\\beta)$\n",
        "\n",
        "- **硬阈值（Hard thresholding）：**当 $r=0$ 时，$L_0(\\beta) = \\sum_{j=1}^p I(\\beta_j \\neq 0)$，其中 $I$ 是指示函数。这相当于执行变量选择，因为它试图最小化非零系数的数量。\n",
        "  \n",
        "- **套索（Lasso）：**当 $r=1$ 时，$L_1(\\beta) = \\sum_{j=1}^p |\\beta_j|$。这导致稀疏解，某些系数将缩减为零，从而进行特征选择。\n",
        "\n",
        "- **岭回归（Ridge Regression）：**当 $r=2$ 时，$L_2(\\beta) = \\sum_{j=1}^p \\beta_j^2$。这将对系数进行平方和惩罚，导致所有系数都缩小，但不会产生稀疏解。\n",
        "\n",
        "- **$L_\\infty$ 正则化：**当 $r=\\infty$ 时，$L_\\infty(\\beta) = \\max_j |\\beta_j|$。这将惩罚绝对值最大的系数，导致一个有界系数的解。\n",
        "\n",
        "### Bridge Estimators 的特点\n",
        "\n",
        "桥接估计量通过变换参数 $r$ 提供了一种统一的正则化方法，它允许我们在不同的正则化策略之间进行选择和切换。$r$ 的不同取值允许模型有不同的复杂度和稀疏性。对于不同的问题，选择合适的 $r$ 可以帮助我们在模型的偏差与方差之间找到最佳的平衡点。\n",
        "\n",
        "### 应用\n",
        "\n",
        "在实际应用中，选择最合适的 $r$ 通常需要根据特定数据集的特性和预测任务的要求来确定。比如，在需要模型解释性时，可能会倾向于使用套索回归；而在关注模型预测准确性时，可能会选择岭回归或者其他合适的 $r$ 值。\n",
        "\n",
        "桥接估计量为我们提供了一个丰富的模型族，通过调整正则化参数和正则化项的类型，可以适应各种不同的数据特性和业务需求。\n"
      ],
      "metadata": {
        "id": "xJW2aaeAM308"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example： 使用普通线性回归、岭回归和套索回归来预测失业率\n",
        "\n",
        "\n",
        "import numpy as np  # 导入numpy库，通常用于进行科学计算\n",
        "import pandas as pd  # 导入pandas库，用于数据处理和分析\n",
        "# 从scikit-learn库中导入数据拆分、线性回归、岭回归和套索回归的函数和类\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 生成模拟的经济数据，用于失业率预测\n",
        "np.random.seed(42)  # 设置随机种子，确保结果的可重现性\n",
        "data = pd.DataFrame({\n",
        "    'GDP': np.random.uniform(1000, 5000, 100),  # 生成GDP数据\n",
        "    'Inflation_Rate': np.random.uniform(1, 5, 100),  # 生成通胀率数据\n",
        "    'Education_Level': np.random.uniform(10, 16, 100),  # 生成教育水平数据\n",
        "    'Average_Income': np.random.uniform(20000, 80000, 100),  # 生成平均收入数据\n",
        "    'Infrastructure_Spending': np.random.uniform(500, 2000, 100),\n",
        "                               # 生成基础设施支出数据\n",
        "    'Unemployment_Rate': 5 + 2 * np.random.randn(100)  # 生成失业率数据\n",
        "})\n",
        "\n",
        "# 将数据拆分为特征(X)和目标变量(y)\n",
        "X = data.drop('Unemployment_Rate', axis=1)  # 除去失业率列，其余作为特征\n",
        "y = data['Unemployment_Rate']  # 将失业率列作为目标变量\n",
        "\n",
        "# 将数据集拆分为训练集和测试集\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
        "                                                    random_state=42)\n",
        "\n",
        "# 普通线性回归\n",
        "linear_model = LinearRegression()  # 创建线性回归模型实例\n",
        "linear_model.fit(X_train, y_train)  # 在训练集上训练模型\n",
        "linear_coefficients = linear_model.coef_  # 获取模型的系数\n",
        "\n",
        "# 岭回归\n",
        "ridge_model = Ridge(alpha=1.0)  # 创建岭回归模型实例，alpha为正则化强度\n",
        "ridge_model.fit(X_train, y_train)  # 在训练集上训练岭回归模型\n",
        "ridge_coefficients = ridge_model.coef_  # 获取岭回归模型的系数\n",
        "\n",
        "# 套索回归\n",
        "lasso_model = Lasso(alpha=1.0)  # 创建套索回归模型实例，alpha为正则化强度\n",
        "lasso_model.fit(X_train, y_train)  # 在训练集上训练套索回归模型\n",
        "lasso_coefficients = lasso_model.coef_  # 获取套索回归模型的系数\n",
        "\n",
        "# 比较三种模型的系数\n",
        "print(\"Ordinary Linear Regression Coefficients:\", linear_coefficients)\n",
        "print(\"Ridge Regression Coefficients:\", ridge_coefficients)\n",
        "print(\"Lasso Regression Coefficients:\", lasso_coefficients)\n",
        "\n",
        "# Ridge regression slightly shrinks the fitting coefficients,\n",
        "# while lasso regression makes the fitting coefficients sparse."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pq7pAmBbPOnq",
        "outputId": "b14f4e1a-fdd3-4c78-82d1-782fc339f3db"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ordinary Linear Regression Coefficients: [ 5.60267613e-04  9.73517555e-02 -8.54416857e-02 -3.02802913e-05\n",
            "  2.78483294e-04]\n",
            "Ridge Regression Coefficients: [ 5.60329468e-04  9.65052229e-02 -8.51602793e-02 -3.02710257e-05\n",
            "  2.78285136e-04]\n",
            "Lasso Regression Coefficients: [ 5.85345112e-04  0.00000000e+00 -0.00000000e+00 -2.78329035e-05\n",
            "  2.45653738e-04]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 梯度下降 Gradient Descent（GD）\n",
        "\n",
        "\n",
        "- 梯度下降是一种优化算法，通常用于机器学习和深度学习中的最小化目标函数。梯度提供了最小化目标函数的方向信息。因此，沿着梯度的相反方向移动可以使我们接近函数的局部最小值。\n",
        "\n",
        "- 假设 $f : \\mathbb{R}^p \\rightarrow \\mathbb{R}$ 是一个 $p$ 元函数。那么梯度下降的形式可以表示为：\n",
        "$$\n",
        "x^{(t)} = x^{(t-1)} - \\lambda \\nabla f (x),\n",
        "$$\n",
        "其中 $\\nabla f(x)$ 是 $f$ 关于 $x$ 的梯度，表示为：\n",
        "$$\n",
        "\\nabla f(x) = \\left( \\frac{\\partial f(x)}{\\partial x_1}, \\frac{\\partial f(x)}{\\partial x_2},..., \\frac{\\partial f(x)}{\\partial x_p} \\right)\n",
        "$$\n",
        "\n",
        "- 当梯度 $\\nabla f (x^{(t)})$ 接近零时，对于足够大的 $t$，我们认为已经达到了函数的最小值附近，此时算法停止。\n",
        "\n",
        "### 梯度下降的应用\n",
        "\n",
        "- 梯度下降通常用于优化问题非凸或者没有解析解，或者当 $x$ 是高维的情况，例如深度神经网络（非凸问题）。\n",
        "- 然而，如果损失函数是凸函数，那么梯度下降算法的输出保证是最优解。\n",
        "- 梯度下降算法的收敛速率会因不同的学习率 $\\alpha$ 选择而有所不同。选择合适的学习率是实现有效优化的关键。在实践中，学习率 $\\alpha$  是一个需要仔细选择的超参数，因为它决定了我们在梯度下降路径上的每一步迈进的距离。过大的学习率可能会导致跳过最小值，而过小的学习率会导致收敛速度过慢。\n",
        "\n",
        "\n",
        "### 梯度下降法的问题\n",
        "\n",
        "1. **冗余计算**：\n",
        "   - 在真实世界的数据中，许多样本可能是相似或冗余的。GD在每次迭代中处理整个数据集，这导致了不必要的计算，而这些计算并不显著地促进学习过程。\n",
        "\n",
        "2. **收敛速度**：\n",
        "   - 使用整个数据集进行每次更新，使得GD的速度变慢，尤其是在数据集很大的情况下。这种缓慢的收敛速度在实践中可能是一个主要缺点。\n",
        "\n",
        "3. **难以逃离局部最小值**：\n",
        "   - 在高维度和复杂的错误景观中（常见于深度学习），尤其是当初始参数值不理想时，GD容易陷入局部最小值或鞍点。"
      ],
      "metadata": {
        "id": "C2DB5zs1kEYl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Original GD 问题的解决方法1: Stochastic Gradient Descent (SGD) 随机梯度下降法\n",
        "\n",
        "与GD不同，SGD不使用整个数据集来计算成本函数的梯度，而是在每次迭代中只使用一个数据点（或一小批数据点）来更新模型参数。在SGD中，随机选择数据集中的一个样本（或一小批样本），基于这个样本计算成本函数的梯度。然后相应地更新模型参数。这个过程对数据集中的每个样本或小批量重复进行。\n",
        "\n",
        "\n",
        "- SGD中的参数更新规则由下面的公式给出：\n",
        "$$\n",
        "\\theta = \\theta - \\alpha \\cdot \\nabla_{\\theta}J(\\theta; x^{(i)}; y^{(i)})\n",
        "$$\n",
        "其中，$\\theta$ 表示模型参数，$\\alpha$ 是学习率，$\\nabla_{\\theta}J(\\theta;x^{(i)};y^{(i)})$ 是损失函数关于参数 $\\theta$ 的梯度，评估在单个数据点 $(x^{(i)}, y^{(i)})$ 上。\n",
        "\n",
        "通过这种方式，SGD每次更新仅依赖于一个样本或一小批样本，可以显著加快收敛速度，同时还有助于模型跳出局部最小值。SGD特别适合于大规模数据集的训练，并且在实践中被广泛应用于深度学习等领域。"
      ],
      "metadata": {
        "id": "xjOiXxpOlPzL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Original GD 问题的解决方法2: Mini-Batch SGD 小批量随机梯度下降**\n",
        "\n",
        "在机器学习中，除了随机梯度下降（SGD）外，还有一种常见的变体叫做小批量随机梯度下降（Mini-Batch SGD）。它是一种折中的优化算法，结合了批量梯度下降和随机梯度下降的优点。\n",
        "\n",
        "- Mini-Batch SGD每次更新不是基于单个样本，也不是整个数据集，而是基于数据集的一个小子集，即“小批量”。\n",
        "\n",
        "- Mini-Batch SGD的参数更新规则如下所示：\n",
        "$$\n",
        "\\theta = \\theta - \\alpha \\cdot \\nabla_{\\theta}J(\\theta; X_{(i:i+n)}, Y_{(i:i+n)})\n",
        "$$\n",
        "这里：\n",
        "- $\\theta$ 表示模型参数。\n",
        "- $\\alpha$ 是学习率，用于控制每一步沿梯度方向更新的步长。\n",
        "- $\\nabla_{\\theta}J(\\theta; X_{(i:i+n)}, Y_{(i:i+n)})$ 是损失函数相对于参数 $\\theta$ 的梯度，这个梯度是在小批量数据上计算的。$X_{(i:i+n)}$ 和 $Y_{(i:i+n)}$ 分别表示从第 $i$ 个数据点到第 $(i+n)$ 个数据点的特征和标签。\n",
        "\n",
        "#### 优势\n",
        "\n",
        "- Mini-Batch SGD相比于单个样本的SGD，能更稳定地向最小值收敛，同时避免了批量梯度下降的大量计算。\n",
        "- 它可以有效地利用高效的矩阵运算，在现代计算框架中常常更加高效。\n",
        "- 同时，因为每一步的更新不是基于整个数据集，这种方法可以减少内存占用，并允许模型在大规模数据集上训练。\n",
        "\n",
        "#### 应用\n",
        "\n",
        "- 在实践中，通常需要实验来确定最优的小批量大小，这取决于问题的性质和计算资源的可用性。小批量大小通常是2的幂次方，如32、64或128，这是因为现代计算库在处理这些大小的数据时更加高效。\n",
        "\n",
        "**Mini-Batch SGD已经成为深度学习实践中的标准优化方法，因为它结合了计算效率和收敛速度的优点，使得在大规模数据集上训练复杂的模型成为可能。**"
      ],
      "metadata": {
        "id": "BZYt1V-unc8F"
      }
    }
  ]
}
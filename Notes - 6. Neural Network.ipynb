{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "线性模型，在回归等早期场景中，我们使用特征值$x_j$与权重$\\beta_j$的加权线性组合来建模关系。然而，当面对更复杂的值关系时，线性模型就显得力不从心。例如，某个范围内的值[0, 5]同样适用，超过8的值被认为是不好的，超过10的并不比刚超过8的更糟糕。这些细微差别需要超越线性组合的建模方法，比如神经网络（Neural Network)。\n",
        "\n",
        "### 神经元(Neurons)激活\n",
        "\n",
        "- 人工神经元（或称麦克洛克-皮茨“单元”）在其输入信号达到某个阈值时会“激发”，类似于生物神经元。\n",
        "\n",
        "### 处理非线性信息\n",
        "\n",
        "- 通过激活函数(Activation Functions)，神经元能够处理复杂的非线性模式和决策边界。激活函数控制着神经元的激活模式。在线性组合之上，我们添加一个非线性函数：\n",
        "  $$\n",
        "  \\hat{y} = \\sigma \\left( \\beta_0 + \\sum_{j=1}^{d} \\beta_j x_j \\right)\n",
        "  $$\n",
        "  \n",
        "### 常见的激活函数$\\sigma(\\cdot)$\n",
        "\n",
        "- Sigmoid函数：$\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
        "- 双曲正切函数：$\\tanh(x)$\n",
        "- ReLU函数：$\\text{ReLU}(x) = \\max(0, x)$\n",
        "\n",
        "### 激活函数的作用\n",
        "\n",
        "- **引入非线性**：激活函数使神经网络能够学习线性模型无法捕捉的复杂模式。\n",
        "- **实现深度学习**：神经网络中的每一层都充当一个独立的处理步骤，多个这样的步骤允许模型建立复杂函数。\n",
        "- **复杂函数的实现**：通过深层架构，神经网络能够实现复杂的函数，捕捉数据中的各种关系。\n",
        "- **约束值范围**：通过激活，层的输出可以被约束在某些期望的范围内。例如，Sigmoid激活函数将输出限制在[0, 1]内，这可以表示概率。\n",
        "\n",
        "通过引入激活函数，神经网络摆脱了线性模型的限制，具备了学习复杂非线性关系的能力。这使得神经网络在图像识别、自然语言处理等需要捕捉高度复杂模式的任务中，表现出色。同时，由于其深度结构，神经网络也能够有效避免维度的诅咒，处理高维数据。"
      ],
      "metadata": {
        "id": "aN91gwGYyNNB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "在神经网络中，每个神经元都可以看作是一个基本单元，它接收输入，对这些输入应用权重，并使用激活函数生成输出。这些神经元被组织成层次结构，使得网络能够通过复杂的权重和激活模式来建模复杂的关系。让我们逐步了解从单个神经元到神经网络的构建。\n",
        "\n",
        "### 单个神经元\n",
        "\n",
        "- **基本工作原理**：每个神经元接收多个输入，每个输入都通过一个权重进行加权，然后这些加权的输入被汇总，并通过一个激活函数来产生输出。\n",
        "\n",
        "- **数学表达**：神经元的输出可以用以下公式表示：\n",
        "  $$\n",
        "  \\text{output} = \\sigma(\\sum_{i} (w_i \\cdot x_i) + b)\n",
        "  $$\n",
        "  其中，$x_i$ 是输入，$w_i$ 是权重，$b$ 是偏置项，$\\sigma$ 是激活函数。\n",
        "\n",
        "### 网络的层次结构\n",
        "\n",
        "- **组织结构**：神经网络通常由多层组成，每一层包含多个神经元。这些层被分为输入层、隐藏层和输出层。\n",
        "\n",
        "- **数据流动**：\n",
        "  - **输入层**：接收原始数据输入。\n",
        "  - **隐藏层**：处理输入层或上一隐藏层的输出。在隐藏层中，神经元对数据进行加权、汇总，并通过激活函数转换。每一层的输出都作为下一层的输入。\n",
        "  - **输出层**：生成最终的预测结果或分类。\n",
        "\n",
        "- **复杂关系建模**：通过在多个层中设置不同的权重和激活函数，神经网络能够捕捉和建模输入数据之间的复杂非线性关系。隐藏层的深度和宽度决定了网络的容量，即其建模复杂函数的能力。\n",
        "\n",
        "神经网络的强大之处在于其能够通过调整层之间的权重，学习输入数据的复杂模式和关系，从而进行有效的预测或分类。随着深度学习的发展，网络结构变得越来越复杂，包括卷积神经网络（CNN）用于处理图像数据，循环神经网络（RNN）用于处理序列数据等，这些都是在基本神经网络结构基础上的扩展和优化。"
      ],
      "metadata": {
        "id": "B_ohB0lJyub_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss Function\n",
        "\n",
        "在神经网络中，损失函数是用来衡量网络预测值$\\hat{y}$与实际目标值$y$之间差异的重要工具。通过最小化损失函数，我们可以学习网络权重$w_j$。下面是对损失函数及其优化方法的详细介绍：\n",
        "\n",
        "### 损失函数\n",
        "\n",
        "- **损失函数的作用**：它是衡量网络预测值与实际目标值之间差异的函数。通过最小化这一函数，可以使得网络预测更加接近真实值。\n",
        "\n",
        "### 常见的损失函数\n",
        "\n",
        "1. **均方误差（Mean Squared Error, MSE）**：\n",
        "    $$\n",
        "    \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2\n",
        "    $$\n",
        "    - 通常用于回归任务，目标是预测尽可能接近目标值的连续值。\n",
        "\n",
        "2. **交叉熵（Cross-Entropy）**：\n",
        "    $$\n",
        "    \\text{Binary Cross-Entropy} = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log(\\hat{y_i}) + (1 - y_i) \\log(1 - \\hat{y_i}) \\right]\n",
        "    $$\n",
        "    - 用于分类任务，尤其是二分类和多类分类，用来衡量两个概率分布之间的差异：输出和目标。\n",
        "\n",
        "### “双重下降现象（Double Descent Phenomenon）”简介\n",
        "\n",
        "- 当我们增加模型复杂度时，训练误差通常会下降，但测试误差会在某一点后上升，这表明过拟合。在某些情况下，继续增加模型复杂度会导致测试误差再次下降，这就是所谓的“双重下降现象”。\n",
        "\n",
        "### 如何优化损失函数\n",
        "\n",
        "- **在简单函数中使用梯度下降（Gradient Descent）**：通过计算损失函数的梯度并沿着梯度的反方向更新权重来最小化损失。\n",
        "\n",
        "- **在神经网络（复合函数）中使用梯度下降和反向传播（Back Propagation）**：\n",
        "    - 反向传播是一种高效计算损失函数关于每个权重梯度的方法，它利用链式法则，使得所有权重都可以朝着减少损失的方向更新。\n",
        "\n",
        "### 为什么使用反向传播（Back Propagation）\n",
        "\n",
        "- **高效学习**：反向传播通过使用链式法则高效地计算损失函数关于每个权重的梯度，从而使得所有权重都可以朝着减少损失的方向更新。\n",
        "- **深度学习能力**：它使得神经网络能够从复杂数据中学习，并执行诸如图像识别、语言翻译（大型语言模型）和玩游戏（强化学习）等高级任务。\n",
        "- **可扩展性**：反向传播方法能够很好地扩展到大型神经网络和数据集，这对于现代深度学习应用至关重要，这些应用涉及大量数据和复杂模型。\n",
        "\n",
        "通过优化损失函数，神经网络能够更好地学习数据中的复杂模式，从而提高预测或分类的准确性。反向传播作为神经网络训练的核心算法之一，其高效和可扩展性使得深度学习能够在多个领域取得显著成就。"
      ],
      "metadata": {
        "id": "AHraA6QD43rT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Neural Networks\n",
        "\n",
        "深度神经网络（Deep Neural Networks, DNNs）指的是包含多个（通常是数百个）隐藏层的神经网络，这些隐藏层位于输入层和输出层之间，使网络能够在多个抽象级别上学习特征。正是这种深度使得深度神经网络能够处理如图像识别、自然语言处理和玩复杂游戏等非常复杂的任务，并且具有显著的准确度。我们之前的示例是包含1个隐藏层的全连接神经网络，即浅层神经网络。下面是对深度神经网络中层的详细介绍：\n",
        "\n",
        "### 深度神经网络的层次结构\n",
        "\n",
        "- **输入层（Input Layer）**：\n",
        "  - 网络的第一层，接收原始输入数据。输入层中的每个神经元代表输入数据的一个特征。\n",
        "  - 原始数据的形式可以是图像数据中的像素、文本数据中的分词，或表格数据的嵌入。\n",
        "\n",
        "- **隐藏层（Hidden Layers）**：\n",
        "  - 位于输入层和输出层之间的层。在DNN中，可以有多个隐藏层，它们是大部分计算发生的地方。\n",
        "  - 每个隐藏层将其输入数据转换成稍微更抽象和复合的表示形式。\n",
        "  - 网络可以包含不同类型的隐藏层，包括：\n",
        "    - **密集层（Dense/Fully Connected Layers）**：一层中的每个神经元都与下一层中的每个神经元相连。\n",
        "    - **卷积层（Convolutional Layers）**：主要用于处理类似网格的数据，如图像。这些层对输入执行卷积操作，捕捉图像或时间序列数据中的空间和时间依赖性。\n",
        "    - **池化层（Pooling Layers）**：用于降低数据的维度，有助于减少计算负担和最小化过拟合。通过对输入数据的块进行汇总来对数据进行下采样。\n",
        "    - **循环层（Recurrent Layers）**：用于处理序列数据，其中前一步的输出被反馈到模型中，以预测序列的结果。这在语言建模和时间序列分析中特别有用。\n",
        "\n",
        "- **输出层（Output Layer）**：\n",
        "  - 产生模型输出的最终层。输出层的设计取决于特定任务（如回归、分类）。对于分类任务，输出层通常使用softmax函数来为每个类别生成概率。\n",
        "\n",
        "深度神经网络通过其多层结构能够捕获输入数据中的复杂模式和关系，使其在多种任务上都能表现出色。每种类型的层都有其特定的作用，从简单特征的提取到复杂模式的识别，这些层共同工作，使得深度神经网络能够实现高级的功能和预测。"
      ],
      "metadata": {
        "id": "o8ZTxR745rKy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 循环神经网络（Recurrent Neural Network, RNN）\n",
        "\n",
        "传统神经网络在处理序列数据和变长输入输出时面临挑战，这些挑战包括数据点之间的依赖性和输入输出长度的可变性。为了克服这些问题，我们使用循环神经网络（Recurrent Neural Networks, RNNs）来改进传统神经网络。以下是对这些概念的详细解释：\n",
        "\n",
        "### 传统神经网络的挑战\n",
        "\n",
        "- **序列数据处理**：传统的前馈网络假设输入之间相互独立，然而在诸如语言翻译或股市预测等实际应用中，数据点的顺序至关重要。\n",
        "\n",
        "- **变长输入输出**：传统神经网络要求固定大小的输入和输出，但在如语音识别等应用中，能够处理变长输入特别有用，因为输入音频片段的持续时间可能差异很大。\n",
        "\n",
        "### 循环神经网络（RNN）\n",
        "\n",
        "- **核心特性**：RNN的最重要特性是其隐藏状态（或称为记忆状态），它记住了序列中的一些信息。这个状态允许网络记住先前的输入。\n",
        "\n",
        "- **参数共享**：RNN在所有输入或隐藏层上执行相同的任务，并使用相同的参数，这降低了参数的复杂性。\n",
        "\n",
        "### RNN的工作原理\n",
        "\n",
        "- RNN由多个固定激活函数单元组成，每个时间步一个。每个单元都有一个内部状态，称为该单元的隐藏状态。\n",
        "  \n",
        "- 隐藏状态代表网络当前持有的关于过去的知识。这个隐藏状态在每个时间步更新，以表示网络关于过去的知识的变化。\n",
        "\n",
        "- 隐藏状态使用以下递归关系更新：\n",
        "  $$\n",
        "  h_t = f(h_{t-1}, x_t)\n",
        "  $$\n",
        "  其中，$h_t$ 是当前状态，$h_{t-1}$ 是前一个状态，$x_t$ 是输入状态。\n",
        "\n",
        "- 例如，使用激活函数（tanh）时，隐藏状态的更新公式为：\n",
        "  $$\n",
        "  h_t = \\tanh(W_{hh}h_{t-1} + W_{xh}x_t)\n",
        "  $$\n",
        "  其中，$W_{hh}$ 是循环神经元的权重，$W_{xh}$ 是输入神经元的权重。\n",
        "\n",
        "- 输出的计算公式为：\n",
        "  $$\n",
        "  y_t = W_{hy}h_t\n",
        "  $$\n",
        "  其中，$y_t$ 是输出，$W_{hy}$ 是输出层的权重。\n",
        "\n",
        "### RNN的优势和劣势\n",
        "\n",
        "- **优势**：\n",
        "  - RNN能够记住每一片信息，并通过时间传递。这在时间序列预测中非常有用，因为它也能记住先前的输入。这被称为长短期记忆（Long Short Term Memory）。\n",
        "  - RNN甚至可以与卷积层一起使用，以扩展有效的像素邻域。\n",
        "\n",
        "- **劣势**：\n",
        "  - 梯度消失和梯度爆炸问题。\n",
        "  - 训练RNN是一个非常困难的任务。\n",
        "  - 如果使用tanh或relu作为激活函数，它不能处理非常长的序列。\n",
        "\n",
        "### 如何处理长期依赖、梯度消失和梯度爆炸\n",
        "\n",
        "- **使用LSTM（长短期记忆网络）**：LSTM是一种在深度学习中广泛使用的RNN架构，擅长捕获长期依赖，非常适合序列预测任务。\n",
        "\n",
        "- **反馈连接**：与传统神经网络不同，LSTM包含反馈连接，使其能够处理整个数据序列，而不仅是单个数据点。这使LSTM在理解和预测时间序列、文本和语音中的模式方面非常有效。\n",
        "\n",
        "### LSTM的逻辑\n",
        "\n",
        "LSTM通过三个主要的门控制单元来处理信息，这些门控制信息的流入、记忆和流出，使得LSTM能够有效地处理长期依赖问题。\n",
        "\n",
        "1. **忘记门（Forget Gate）**：\n",
        "   - 忘记门决定了从单元状态中丢弃哪些信息。它查看当前输入$x_t$和前一个隐藏状态$h_{t-1}$，为单元状态$C_{t-1}$中的每个数字输出一个介于0和1之间的数。0表示“完全忘记”，而1表示“完全保留”。\n",
        "   - 忘记门的决策是通过以下公式做出的：\n",
        "     $$\n",
        "     f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)\n",
        "     $$\n",
        "     其中，$W_f$是忘记门的权重，$b_f$是偏置项，$\\sigma$表示Sigmoid函数，确保输出值在0到1之间。\n",
        "\n",
        "2. **输入门（Input Gate）**：\n",
        "   - 输入门决定了将哪些新的信息存储在单元状态中。它首先使用一个Sigmoid层来决定哪些值需要更新，然后使用一个tanh层来创建新的候选值向量$C_t$，这些值可以被添加到状态中。\n",
        "   - 单元状态的更新公式为：\n",
        "     $$\n",
        "     i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)\n",
        "     $$\n",
        "     $$\n",
        "     \\tilde{C_t} = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)\n",
        "     $$\n",
        "     其中，$i_t$是输入门的输出，决定了哪些值要更新，$\\tilde{C_t}$是候选值向量。\n",
        "\n",
        "3. **输出门（Output Gate）**：\n",
        "   - 输出门决定了下一个隐藏状态的值，隐藏状态包含关于先前输入的信息。隐藏状态也用于预测或决定下一个输出。\n",
        "   - 输出门的操作可以通过以下公式进行：\n",
        "     $$\n",
        "     o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)\n",
        "     $$\n",
        "     $$\n",
        "     h_t = o_t * \\tanh(C_t)\n",
        "     $$\n",
        "     其中，$o_t$是输出门的激活函数，$C_t$是当前时刻的单元状态，$h_t$是当前时刻的隐藏状态。\n",
        "\n",
        "通过这些机制，LSTM能够记住重要的信息，并在需要时保留或丢弃信息，从而有效地处理序列数据中的长期依赖问题。这使得LSTM在许多涉及序列预测的深度学习应用中成为首选架构，如语言模型、时间序列预测和语音识别等。\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "G_5rpVs8CYtr"
      }
    }
  ]
}